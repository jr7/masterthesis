\chapter[ GPGPU-Based Implementation of a finite difference algorithm]{GPGPU-Based Implementation of \\ a finite difference algorithm}


The objective of this chapter is to give an overview of the basic implemenation of an algorithm using finite difference schemes.
The source code used for the computations is based on an existing and validated version from A. Tilgner.
It was furthermore extended by the immersed boundary methods, which will be explained in more detail in chapter \ref{CHAPTER:IBM}.
In this context an introduction to the aspects of GPGPU\footnote{General Purpose Computation on Graphics Processing Uni}-Computing
with the CUDA \footnote{Computing Uniform Device Architecture} architecture will be given.
For the computations in this thesis Tesla C1060 and Tesla K20m GPUs by NVIDIA were used.%, the complete system configurations can be looked up at (AX.X).
In this chapter, all informations related to GPGPU-Computing and CUDA are adapted from the CUDA Programming Guide and the CUDA Best Practices Guide from NVIDIA
\citep{CUDAPG}, \citep{CUDABP}. All details about the implementation of the algorithm were obtained by studying the source code
and from private communication with A.Tilgner.

\section{GPGPU-Computing with CUDA}

CUDA is an architecture developed by NVIDIA to enable an easy approach to the implementation of GPGPU-based algorithms.
The underlying idea is to hide the complexity of the hardware under a more high oriented and generalized software abstraction layer.
This is done by introducing some additional programming language extensions for example in C/C++,
furthermore it is necessary to use a CUDA suited compiler like NVCC.

\subsection{Hardware and Memory Architecture}

A cuda device contains an array of so called streaming multiprocessors (SM).
Each of these SMs contains a group of small execution units, which are called cuda cores.
For the exchange of data between different SMs, cuda cores and the CPU side of the computer, there exists different
types of memory, as shown  in Fig.  \ref{fig:gpu_memory_layout}.
\newpage

\begin{figure}[!tbp]
      \centering
        \includegraphics[width=0.9\textwidth]{gfx/cuda/gpu.pdf}
          \caption{Memory layout of a Nvidia-GPU}
    \label{fig:gpu_memory_layout}
\end{figure}

\begin{description}
    \item[Global Memory] The global memory is the largest memory on the gpu and can be accessed by all cuda cores.
                         It is  used to exchange data with the RAM and distribute it between different SMs.
                         Writing and reading from the global memory is slow. A part of the global memory can furthermore be used as a constant memory.
                         In this case multiple memory accesses on the same location are temporary cached.
                         In addition the use of the global memory as a texture memory is possible. The memory access is then spatially cached.


    \item[Shared Memory] Each SM contains a shared memory, much smaller than the global memory \footnote{Around 16kb on c1060 and 64kb on k20m}. It is accessible
                         by all cuda cores of the SM.
                         This memory type is much faster (x100) than the global memory and is usefull when multiple operations
                         by different cuda cores are carried out on the same data.

    \item[Register / Local Memory] Each cuda core posseses an own set of registers and local memory.
                                   The register access is slightly faster than the access  on the shared memory.
                                   Local memory access is very slow since
                                   it is created on the global memory, when the cuda core runs out of resources.
\end{description}

\clearpage

\subsection{Thread Management}

In order to distribute the workload between different SMs and cuda cores an additional syntax is necessary.
The CUDA API therefore  introcuces the notion of a thread and a block.

\begin{description}
    \item[Thread] A Thread is the software abstraction of a single cuda core. It can be identified with
                   a maximum of three IDs, \textbf{threadIdx.x}, \textbf{threadIdx.y} and\textbf{threadIdx.z}.

    \item[Block] A block is defined as a group of threads, i.e. in Fig. \ref{cuda:grid_example} a Block contains of 64 threads.
                 There  is no direct hardware equivalent to a block.
                 A block is identified by a maximum of two IDs, \textbf{blockIdx.x} and \textbf{blockIdx.y}

    \item[Blocksize] The blocksize is defined before running a function on the gpu. It determines how
                    many threads run inside a block. In Fig. \ref{cuda:grid_example} the blocksize is set to (8, 8) and
                    each thread has a threadIdx.x and threadIdx.y.

    \item[Gridsize] The gridsize defines the total number of blocks, i.e. (2, 2) in the example.
\end{description}

The concept of a grid containing blocks of threads is a little bit confusing at first.
However these abstractions are necessary to enable a high workload on the gpu,
since each block can be dynamically assigned to a different SM.


\begin{figure}[!bp]
      \centering
        \resizebox{0.5 \textwidth}{!}{
       \import{gfx/cuda//}{grid.pdf_tex}
        }
       \caption{Segmentation of the $y, z$ plane of a simulation domain into a grid of Blocks.
                 Eeach block contains a grid with 8x8 threads.}
       \label{cuda:grid_example}
\end{figure}

\subsection{Performance Considerations}

In order to obtain a high performant algorithm
For the opti
For a cuda devices different optimization strategies are recommended,

\begin{description}
    \item[Coalesced Readings]
    \item[Sparse global memory acces]{
        The global memory should be used for data transfer and distribution, with as little as possible global reads.
        Immediatly after the data is transfered to the shared memory there should be as many as possible computations before transfering
        it back. Furthermore all memory accesses should be optimized, see chapter (X.X).}
    \end{description}


\subsection{Simulation and  Memory Layout}

All simulations in this thesis are performed on a three-dimensional cartesian grid
with a constant grid size  in each dimension.
The simulation domain is given by a cube with with lengths $l_x, l_y, l_z$
and the number of grid points is set by $N_x$, $N_y$ and $N_z$ in each direction.
For the imlempentation of the boundarie conditions  two additional layer of grid points
are added on each side of the simulation domain.
The additonal points, which are referred to as ghostpoints \citep{ctie},
are computed after each timestep in order to fulfill the boundarie conditions.

For each variable three different memory allocations are necessary
For the imlempentation of the boundarie conditions  two additional layer of grid points
are added on each side of the simulation domain.




-dimensions etc
-ghost points etc

\subsubsection{Boundaries}


\label{cuda:prepro}


The first allocation will be one the system RAM.

The second and third allocation reside on the global memory of the gpu.
This is due to the fact that the runge kutta method uses two storage registers, during the timestep execution.
The fields for all variables are summarized in table (1).

\begin{center}
    \begin{tabular}{ | l | l | l | l |}
    \hline
    Variable & Field on RAM & GPU Field & 2nd GPU Field \\
    \hline
    Velocity in x-direction $v_x$  & \texttt{vx}   &  \texttt{vx\_d}   & \texttt{vx2\_d}   \\
    Velocity in y-direction $v_y$  & \texttt{vy}   &  \texttt{vy\_d}   & \texttt{vy2\_d}   \\
    Velocity in z-direction $v_z$  & \texttt{vz}   &  \texttt{vz\_d}   & \texttt{vz2\_d}   \\
    Density  $\rho$  & \texttt{rho}  &  \texttt{rho\_d}  & \texttt{rho2\_d}  \\
    \hline
    \end{tabular}
\end{center}
The parameter set defines different attributes of the system.
This can be physical attributes like the rayleigh- or prandtl-number, numerical parameters like the grid resolution
memory layout

\clearpage


\begin{figure}[!bp]
      \centering
        \resizebox{0.6\textwidth}{!}{
       \import{gfx/cuda//}{stencil.pdf_tex}
        }
       \caption{Segmentation of the $y, z$ plane of the simulation domain into a grid of Blocks.
                 Eeach block contains a grid with 8x8 threads.}
       \label{cuda:stencil}
\end{figure}

\begin{figure}[!bp]
      \centering
        \resizebox{0.6\textwidth}{!}{
       \import{gfx/cuda//}{timestep.pdf_tex}
        }
       \caption{
           Memory location during a time step in the $y, z$ plane.
           }
       \label{cuda:timestep_algo_img}
\end{figure}

\clearpage
\section{Time Step Algorithm}

In this section an overview of the computation of a single time step will be given.
The simulation domain is a cube of length $l$ and the resolution $N$ in three dimensions.
For this example the Navier-Stokes equation given by Eq. () will be used.
It is presumed that all variables are allocated on the global memory of the gpu, see Sec. \ref{cuda:prepro}.
The simulation domain is then divided into a grid of  $N/8 \times N/8$ blocks, like in Fig. \ref{cuda:grid_example}.
Each block contains a total number of 8x8x4 threads.
The ID threadIdx.z defines, which variable will be used for the computation, for example threadIdx.z=0 corresponds to $\rho$ and threadIdx.z=1 to $v_x$.

exemplary one block

For the temporal integration the Willamson-RK3 scheme from Sec. () is used.
This schemes consist of total  six sub steps here only the first two will be considered thus

\begin{align}
    \begin{split}
    Q_1 = \Delta t \mathcal{L}^*\left(\Phi^n\right)\qquad &\Rightarrow \qquad \Phi^{1} = \Phi^n + \frac{1}{3}Q_1
    \end{split}
\end{align}
the remaining computations are identical except the use of different coefficients.

For a computation of the finite difference schemes the simulation uses a 5-Point stencil as shown in Fig. \ref{cuda:stencil}.
The inner points of the stencil are enumerated with 1, the outer with 2., for example in $x$ direction the points are denoted as West2,
West, East2, East in $x$ direction or in short (W2, W1, E1, E2).

\footnote{For second order finite difference schemes, only the inner points are used, we refer to a 3-Point stencil in this case.}


The computation of one timestep can be seperated into an initialzation part and a for-loop which iterates over the simulation domain.
As a first step the position, indexed with $ix, iy, iz$ is set to

\begin{verbatim}
   ix=2;
   iy=blockIdx.x*BLOCKSIZE+threadIdx.y+2;
   iz=blockIdx.y*BLOCKSIZE+threadIdx.x+2;
\end{verbatim}

The offset 2 is a result  of the boundarie layers, more on this in Sect. ()
Each thread in the block is then assigned to the grid point $C = P(ix, iy, iz)$.
The position of the center $C$ can be seen in Fig. ().

In the next step each thread has to load the neighboor points W2, W1, E1 and E2,
with the position given by an offset with a multiple of $(N+4)^2$, the number of grid points in one layer.
Beside these points, additional points for $ix=2$ have to been loaded on the side of the block.

After this initialization a whole stack of 8x8x5 is loaded into the shared memory for each variable.
Fig (), shows the position in the simulation domain, the 5-Point stencil is exemplariliy shown for one center point,
furthermore the (ghost?) points on the sides are shown.

The initialization is now finished. In the first step of the for-loop the spatial derivatives arecomputed.
Here with the  use of fourth order finite difference schemes and the upwinding method of third order.
The following abreviations are defined:

\begin{align}
    LP (L, R, \Phi, \Delta)  &:=  \frac{1}{12\Delta x^2} \left(-\Phi_{R2} + 16\Phi_{R1} - 30\Phi_C +16\Phi_{L1} - \Phi_{L2} \right)\\
    DP (L, R, \Phi, \Delta)  &:=  \frac{1}{12\Delta x}   \left(-\Phi_{R2} + 8\Phi_{R1}  -8\Phi_{L1} + \Phi_{L2} \right)\\
    V_-(L, R, \Phi, \Delta)  &:=  \frac{1}{6\Delta x}  \left( 2\Phi_{R1} + 3\Phi_C  - 6\Phi_{L1} + \Phi_{L2}  \right)\\
    V_+(L, R, \Phi, \Delta)  &:=  \frac{1}{6\Delta x}  \left( -\Phi_{R2} + 6\Phi_{R1}  - 3\Phi_{C} + \Phi_{L1}  \right)
\end{align}

L and R are two oppositle directions of the 5-Point stencil and $\Phi$ a variable of the fluid.

\clearpage

Since the variable itself is defined by threadIdx.z the computation with the discretized operatior is given by

\begin{verbatim}
      if (threadIdx.z == 0) //rho
      {
         rhs=-DERIV_X(vx_s)-DERIV_Y(vy_s)-DERIV_Z(vz_s);
      }
      if (threadIdx.z == 1)//vx
      {
         ADVECT(vx_s)
         rhs=rhs-c2*DERIV_X(rho_s)
   +(1.0/Ra)*LAPLACE(vx_s);

      if (threadIdx.z == 2) //vy
      {
         ADVECT(vy_s)
         rhs=rhs-c2*DERIV_Y(rho_s) +(1.0/Ra)*LAPLACE(vy_s);

      if (threadIdx.z == 3) //vz
      {
         ADVECT(vz_s)
         rhs=rhs-c2*DERIV_Z(rho_s)+Pr*LAPLACE(vz_s)+Pr*Ra*temp_s[Point];
      }
\end{verbatim}

stored in new register

\subsection{Boundary Conditions}

For the imlempentation of the boundarie conditions from Sec. () two additional layer of grid points
are added on each side of the simulation domain.
The total number of grid points is ${(N_x + 4)\cdot(N_y + 4)\cdot(N_z + 4)}$.
The additonal points, which are referred to as ghostpoints \citep{ctie},
are computed after each timestep in order to fulfill the boundarie conditions.
For the purpose of explanation we will here consider the one-dimensional case,
for finite differences of fourth order. (The methods were given by atilgner.)

v?

\paragraph{No-Slip Boundaries}\mbox{}\\

The left boundarie is set to the point $b=2$,
the ghost points arg given by $g_2=0$ and $g_1=1$, the inner points close
to the boundarie are $l_1=3$ and $l_2=4$.
In order to obtain No-Slip boundaries the velocity profile is mirrored and inverted
at $b$. Thus, $v(g_2) = -v(l_2)$ and  $v(g_1) = -v(l_1)$



\paragraph{No-Flux and Free-Slip Boundaries}\mbox{}\\

The left boundarie is set to the point $b=2$,
the ghost points arg given by $g_2=0$ and $g_1=1$, the inner points close
to the boundarie are $l_1=3$ and $l_2=4$.
In order to obtain No-Slip boundaries the velocity profile is mirrored and inverted
at $b$. Thus, $v(g_2) = -v(l_2)$ and  $v(g_1) = -v(l_1)$


\paragraph{Periodic Boundaries}\mbox{}\\

The left boundarie is set to the point $b=2$,
the ghost points arg given by $g_2=0$ and $g_1=1$, the inner points close
to the boundarie are $l_1=3$ and $l_2=4$.
In order to obtain No-Slip boundaries the velocity profile is mirrored and inverted
at $b$. Thus, $v(g_2) = -v(l_2)$ and  $v(g_1) = -v(l_1)$

\subsection{Remarks}

-performance blabla
-tilgner bsp
-threads

The position of the points in the shared memory is defined by an offset
The offset of these points to the current one can be computed by

-store date in register

ix wird aufadiert points are shiftet  and the next layer is loaded (img nachtragene)
the procuder is fertig wenn simulation domain  durchgearbetit ist .

\clearpage

\subsection{Boundary Conditions}

For the imlempentation of the boundarie conditions from Sec. () two additional layer of grid points
are added on each side of the simulation domain.
The total number of grid points is ${(N_x + 4)\cdot(N_y + 4)\cdot(N_z + 4)}$.
The additonal points, which are referred to as ghostpoints \citep{ctie},
are computed after each timestep in order to fulfill the boundarie conditions.
For the purpose of explanation we will here consider the one-dimensional case,
for finite differences of fourth order. (The methods were given by atilgner.)

v?

\paragraph{No-Slip Boundaries}\mbox{}\\

The left boundarie is set to the point $b=2$,
the ghost points arg given by $g_2=0$ and $g_1=1$, the inner points close
to the boundarie are $l_1=3$ and $l_2=4$.
In order to obtain No-Slip boundaries the velocity profile is mirrored and inverted
at $b$. Thus, $v(g_2) = -v(l_2)$ and  $v(g_1) = -v(l_1)$



\paragraph{No-Flux and Free-Slip Boundaries}\mbox{}\\

The left boundarie is set to the point $b=2$,
the ghost points arg given by $g_2=0$ and $g_1=1$, the inner points close
to the boundarie are $l_1=3$ and $l_2=4$.
In order to obtain No-Slip boundaries the velocity profile is mirrored and inverted
at $b$. Thus, $v(g_2) = -v(l_2)$ and  $v(g_1) = -v(l_1)$


\paragraph{Periodic Boundaries}\mbox{}\\

The left boundarie is set to the point $b=2$,
the ghost points arg given by $g_2=0$ and $g_1=1$, the inner points close
to the boundarie are $l_1=3$ and $l_2=4$.
In order to obtain No-Slip boundaries the velocity profile is mirrored and inverted
at $b$. Thus, $v(g_2) = -v(l_2)$ and  $v(g_1) = -v(l_1)$

\subsection{Remarks}

-performance blabla
-tilgner bsp
-threads

- measurements / output
- zusammenfassung flow diagramm
- erläuterung threads
- python api

%During execution time each cuda core is executed as a thread, in an SIMD\footnote{Single Instruction Multiple Device}-like behaviour.
%This means every cuda core on one SM executes the same instruction set simultaneously. To differentiate between different cuda cores,
%each get assigned to a different \textbf{ThreadId}.
%With the declaration of a block dimension, we define how many threads are executed in parallel.
%The block dimension can be up to 3-dimensions. In this case each cuda core will be assigned to a ThreadId in x, y and z direction.
%A collection of threads with this grid structure is called a block.
%Since the


