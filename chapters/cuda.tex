\chapter{GPGPU-Based Implementation of a finite difference algorithm}


Now that we have given an introduction to the theoretical and numerical aspects of the thesis,
the object of this section is to give an overview of the implemenation of an algorithm using finite difference schemes.
The source code used for the computations is based on an existing version from [A.Tilgner].
It was furthermore optimized  and extended by the immersed boundary methods, which will be explained in more detail in chapter (5.).
Especially in this context we will introduce some aspects of GPGPU\footnote{General Purpose Computation on Graphics Processing Unit - Allg.  Bezeichung für Allzweck-Berechnungen auf Grafikprozessoren}-Computing
with the CUDA \footnote{Computing Uniform Device Architecture} architecture.
For the computations the  Tesla C1060 and Tesla K20m GPUs by NVIDIA were used, the complete system configurations can be looked up at (AX.X).

\section{GPGPU-Computing with CUDA}

CUDA is an architecture developed by NVIDIA to enable an easy approach to the Implementation of GPGPU-based algorithms.
The underlying idea is to hide the complexity of the hardware under a more high oriented and generalized software abstraction layer.
This is done by introducing some additional programming language extensions i.e. in c/c++\footnote{WIR BENUTZEN C/C++ /PYTHON ETC},
furthermore it is necessary to use a CUDA suited compiler like NVCC.

\subsection{Hardware and Memory Architecture}

A cuda device contains an array of so called streaming multiprocessors (SM).
Each of these SMs contains a group of small execution units, which are called cuda cores.
For the exchange of data between different SMs, cuda cores and the CPU side of the computer, there exists different
types of memory, as shown  in figure (X).
\newpage

\begin{figure}[!tbp]
  \centering
  \includegraphics[width=0.8\textwidth]{gfx/cuda/gpu.png}\label{fig:gpu_arch}
  \caption{Memory layout of a Nvidia-GPU}
\end{figure}

\begin{description}
    \item[Global Memory] The global memory is the largest memory on the gpu and can be accessed by all cuda cores.
                         It is  used to exchange data with the RAM and distribute it between different SMs.
                         Reading from the global memory can be quite slow, therefore different optimization approaches exists,
                         for example the creation of a cached texture- or constant-memory.

    \item[Shared Memory] Each SM contains a shared memory, much smaller than the global memory \footnote{Around 16kb on c1060 and 64kb on k20m}. It is accessible
                         by all cuda cores of the SM.
                         This memory type is much faster (x100) than the global memory and is really usefull when multiple operations
                         by different cuda cores are carried out on the same data.

    \item[Register / Local Memory] Each cuda core posseses an own set of registers and local memory.
                                   The register access is even slightly faster as on the shared memory. The local memory access is very slow since
                                   it is created on the global memory, when the cuda core runs out of resources.
\end{description}

As a conlusion the algorithm should fulfil the following prerequisites.
The global memory should be used for data transfer and distribution, with as little as possible global reads.
Immediatly after the data is transfered to the shared memory there should be as many as possible computations before transfering
it back. Furthermore all memory accesses should be optimized, see chapter (X.X).

\subsection{CUDA C/C++ API}

The CUDA C/C++ API is an extension of the C programming language.
There exists a different number of versions, supporting a different number of hardware architectures, def by C.
The ???BLABLAalgorithm in our cases uses the compute capability X.X.
A majority of the API functions are so called \textbf{host functions}  which are executed by the CPU.
These functions are used i.e. for storage allocation on the gpu, data transfers and error checking.
For example the function calls
\begin{verbatim}
    cudaMalloc((void **) &vx_d, A);
    cudaMemCpy((void **) &vx_d, A);
    cudaMemCpy((void **) &vx_d, A);
\end{verbatim}
allocates memory with size A at the pointer vx\_d on the device (GPU), then copy it to the device then free it . As we can see here, the syntax for host functions is still in C.
There are two kinds of function types executed on the device.
The most import are \textbf{global functions} also called kernel, which are called by the host and executed on the device.
Last but not least there are \textbf{device functions} which are called and executed by the device.
An example of the syntax for defining these kind of functions is given in Appendix (X.X).
For the parallelization of a kernel the CUDA API introduces a new syntax, i.e.

\begin{verbatim}
    time_step<<<dimGrid,dimBlock>>>(x, y, z, ...);
\end{verbatim}

executes the kernel function \texttt{time\_step} on the device in parallel.
The new syntax element is the bracket \texttt{<<<dimGrid, dimBlock>>>}.

NEUSCHREIBEN
It defines the dimension of the \textbf{grid} and the \textbf{block} of a kernel:
During execution time each cuda core is executed as a thread, in an SIMD\footnote{Single Instruction Multiple Device}-like behaviour.
This means every cuda core on one SM executes the same instruction set simultaneously. To differentiate between different cuda cores,
each get assigned to a different \textbf{ThreadId}.
With the declaration of a block dimension, we define how many threads are executed in parallel.
The block dimension can be up to 3-dimensions. In this case each cuda core will be assigned to a ThreadId in x, y and z direction.
A collection of threads with this grid structure is called a block.
Since the

The grid dimension describes the
NEUSCHREIBEN

\begin{figure}[!tbp]
  \centering
  \includegraphics[width=0.8\textwidth]{gfx/cuda/gpu.png}\label{fig:gpu_grid}
  \caption{Block and grid layout for a two dimensional array.}
\end{figure}

To illustrate the usage of grid and block dimensions, let us consider a two-dimensional array of shape 16x16, as shown in figure ().
-copy to gpu
-1dimensional structure

In this example \textbf{dimGrid} and \textbf{dimBlock} would be defined by
\begin{verbatim}
    dim3 dimBlock(2, 2);
    dim3 dimGrid(2, 2);
    int blocksize = 2;
    blocksize = ...
\end{verbatim}

A kernel in which each thread accesses exactly one element of the matrix, would have the following form

\begin{verbatim}
    __global__ void example(vx_array){
        int id; float c;
        id = blocksize*BlockIdx.x + blocksize*BlockIdx.y +  threadId.x + threadId.y;
        //read from vx_array
        c = vx_array[i];
        //do some computations ... .
        c += 2;
        //write output back to vx_d
        vx_array[id] = c;
\end{verbatim}

\subsection{Optimization}
- coalesceded
- bank conflicts?
- teilvolumen nicht rechnen
\newpage

\section{Implementation}

Finally we will explain the implemented algorithm in detail.
As an example the simulation of a Rayleigh-Benard system with a finite difference scheme of fourth order
and the third order runge kutta method is  used.
During execution time the algorithm can be seperated in three parts.

\subsection{Pre processing}

Before integrating the equations of motion we need to define the initial state of our system, as stated in section (X)
This includes the flow variables but also a set of parameters.
For one flow variable we need three different memory allocations.
The first allocation will be one the system RAM.
-hdf5 python


-loading with python api
- from here one copy to gpu memcpytodevice
The second and third allocation reside on the global memory of the gpu.
This is due to the fact that the runge kutta method uses two storage registers, during the timestep execution.
The fields for all variables are summarized in table (1).

\begin{center}
    \begin{tabular}{ | l | l | l | l |}
    \hline
    Variable & Field on RAM & GPU Field & 2nd GPU Field \\
    \hline
    Velocity in x-direction $v_x$  & \texttt{vx}   &  \texttt{vx\_d}   & \texttt{vx2\_d}   \\
    Velocity in y-direction $v_y$  & \texttt{vy}   &  \texttt{vy\_d}   & \texttt{vy2\_d}   \\
    Velocity in z-direction $v_z$  & \texttt{vz}   &  \texttt{vz\_d}   & \texttt{vz2\_d}   \\
    Density  $\rho$  & \texttt{rho}  &  \texttt{rho\_d}  & \texttt{rho2\_d}  \\
    Temperature $T$ & \texttt{temp} &  \texttt{temp\_d} & \texttt{temp2\_d} \\
    \hline
    \end{tabular}
\end{center}

The parameter set defines different attributes of the system.
This can be physical attributes like the rayleigh- or prandtl-number, numerical parameters like the grid resolution
or simple flags for setting the boundary conditions. A list of all possible parameters can be found in Appendix A1.X


-table vx2  etc
-table with constants
-explanation config file python api

- preloading defining constants
- 2 arrays

-move to gpu
-memcpy to symbol speed

\subsection{Main Simulation Loop}
- the discretized equations are ..
- with this in mind we need at least a 4 point stencil

- we could use threadid.z for 3d but instead

- picture stencil in action

- for loop

\subsection{Post processing}
- measurements / output

- zusammenfassung flow diagramm

- erläuterung threads

\section{Python API}

\section{Validierung}
- beispiel rayleigh benard system
- masa
- vgl o2 vs o4 masa cube
- bifurcation

\newpage

